{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpdG99uzA_IN"
   },
   "source": [
    "**Purpose:**\n",
    "This notebook should consist of our methods of detecting faces and adding effects on top on them in a video. \n",
    "It is written in stages on Google Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T02:09:08.855105Z",
     "start_time": "2021-05-04T02:09:02.561765Z"
    },
    "id": "FYuvpb2BCLzr"
   },
   "outputs": [],
   "source": [
    "!pip install face_recognition\n",
    "!pip install facenet_pytorch\n",
    "!pip install mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T17:45:34.219545Z",
     "start_time": "2021-05-04T17:45:34.200672Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OuKTGOu5A_IY",
    "outputId": "b36e9a18-9d3a-400c-aa42-e8e390763f11"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline \n",
    "import face_recognition\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import imageio\n",
    "from itertools import product\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# import cupy as cp\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T01:30:25.028342Z",
     "start_time": "2021-05-04T01:30:25.023451Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWY0mHJSA_Ib",
    "outputId": "d68fdd13-6164-4876-f8d3-d583c03683d9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "run_on_colab = True\n",
    "if run_on_colab:\n",
    "    from google.colab import drive\n",
    "  \n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    path = \"/content/drive/My Drive/AdvancedPython2021\"\n",
    "else:\n",
    "    path = os.getcwd()\n",
    "os.chdir(path)\n",
    "from array_mp4_conversion import array_to_mp4, mp4_to_array\n",
    "file = \"girl.gif\"\n",
    "data_path = os.path.join(path, \"data\", file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T01:30:25.574502Z",
     "start_time": "2021-05-04T01:30:25.555058Z"
    },
    "code_folding": [],
    "id": "QAfO0x-2A_Ib"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import imageio\n",
    "from itertools import product\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "from array_mp4_conversion import array_to_mp4, mp4_to_array\n",
    "from numba import jit, prange, cuda\n",
    "from collections import defaultdict\n",
    "from scipy.signal import convolve2d\n",
    "# cimport numpy as np\n",
    "# ctypedef np.uint8_t D_TYPE\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "class video_transformer_base:\n",
    "    '''\n",
    "    This is the base of video_transformer, containing basic information about the video.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                path, \n",
    "                save_path, \n",
    "                file_name, \n",
    "                device='cpu',\n",
    "                display=False):\n",
    "        \n",
    "        self.video_path = os.path.join(path, \"data\", file_name)\n",
    "        self.video_array, self.fps = mp4_to_array(self.video_path)\n",
    "        self.display = display\n",
    "        self.save_path = save_path\n",
    "        self.file_name = file_name\n",
    "        self.num_frames = 0\n",
    "        if device == 'cpu':\n",
    "            self.device = 'cpu'\n",
    "        else:\n",
    "            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def main_transformation(self, \n",
    "                            face_detection_model, \n",
    "                            filter_effect):\n",
    "        '''\n",
    "        For each frame, do:\n",
    "        1. detect the face;\n",
    "        2. apply the filter;\n",
    "        3. save the processed frame.\n",
    "        '''                    \n",
    "        video_capture = cv2.VideoCapture(self.video_path)\n",
    "        frame_count = 0\n",
    "        output_frames = []\n",
    "        while video_capture.isOpened():    \n",
    "            # Grab a single frame of video\n",
    "            ret, frame = video_capture.read()\n",
    "\n",
    "            # Bail out when the video file ends\n",
    "            if not ret:\n",
    "                video_capture.release()\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "\n",
    "            # detect faces\n",
    "            if face_detection_model != \"mtcnn\":\n",
    "                face_locations = self.face_detection(frame, \n",
    "                                                     model=face_detection_model)\n",
    "            else:\n",
    "                face_locations = self.face_detection_mtcnn(frame)\n",
    "\n",
    "            # add effect\n",
    "            after_effect_frame = filter_effect(frame, face_locations)\n",
    "\n",
    "            if self.display and frame_count % 2 == 0:\n",
    "               # If faces were found, we will mark it on frame with blue dots\n",
    "                for face_location in face_locations:\n",
    "                    top, right, bottom, left = face_location\n",
    "                    cv2.rectangle(after_effect_frame,(left, top), (right, bottom), (0, 0, 255), 2)\n",
    "                plt.imshow(after_effect_frame)\n",
    "                plt.show()\n",
    "                clear_output(wait=True)\n",
    "\n",
    "            output_frames.append(after_effect_frame)\n",
    "        self.num_frames = frame_count\n",
    "        self.des_arr = np.array(output_frames)\n",
    "\n",
    "    def face_detection(self, frame):\n",
    "        '''\n",
    "        Face detection with package face_recognition.\n",
    "        Models includes: svm, knn, cnn.\n",
    "        Currently fixed as model='svm' because model ='cnn' is slow.\n",
    "        '''\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rgb_frame = frame[:, :, ::-1]\n",
    "        face_locations = face_recognition.face_locations(rgb_frame, model='svm')\n",
    "        # print(f\"{len(face_locations)} face(s) detected.\")\n",
    "        \n",
    "        return face_locations\n",
    "    def face_detection_mtcnn(self, frame):\n",
    "        '''\n",
    "        Face detection with package facenet_pytorch.\n",
    "        MTCNN implemented in Pytorch, so also support CUDA.\n",
    "        '''       \n",
    "\n",
    "        mtcnn = MTCNN(keep_all=True, device=self.device)\n",
    "        boxes, _ = mtcnn.detect(frame)\n",
    "        \n",
    "        if boxes is None:\n",
    "            boxes = []\n",
    "            \n",
    "        boxes = np.array([[box[1], box[2], box[3], box[0]] for box in boxes]).astype(np.int)\n",
    "        # print(f\"{len(boxes)} face(s) detected.\")\n",
    "        return boxes\n",
    "    def oil_effect(self, frame):\n",
    "        '''\n",
    "        Please refer to unused_oil_effect.py for implementation.\n",
    "        '''\n",
    "        \n",
    "        pass\n",
    "    \n",
    "    def negative_effect(self, frame, locations):\n",
    "        '''\n",
    "        Apply negative filter effect to target locations.\n",
    "        '''\n",
    "        des_img = np.copy(frame)\n",
    "        try:\n",
    "            for location in locations:\n",
    "                t_, r_, b_, l_ = location.astype(int)\n",
    "\n",
    "                des_img[t_:b_,l_:r_] = 255 - frame[t_:b_,l_:r_]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return des_img\n",
    "\n",
    "    def mean_blur(self, frame, locations, radius=5):\n",
    "        '''\n",
    "        Apply simple mosaic effect to specified regions. \n",
    "        '''\n",
    "        k = 1 / (radius*2+1)**2\n",
    "        des_img = np.copy(frame)\n",
    "        height, width, _ = des_img.shape\n",
    "\n",
    "        for location in locations:\n",
    "            top, right, bottom, left = location\n",
    "            t_ = max(top+radius,0)\n",
    "            b_ = min(bottom-radius, height)\n",
    "            l_ = max(left+radius,0)\n",
    "            r_ = min(right-radius, width)\n",
    "            if t_ >= b_ or l_ >= r_:\n",
    "                continue\n",
    "\n",
    "            for i, j in product(range(t_, b_), range(l_, r_)):\n",
    "                kernel = frame[i-radius:i+radius+1, j-radius:j+radius+1, :]\n",
    "                sumed = np.sum(kernel, axis = (0,1)) * k\n",
    "                des_img[i, j] = sumed.astype(np.uint8)\n",
    "\n",
    "        \n",
    "        return des_img    \n",
    "    \n",
    "    def write_to_video(self, output_filename):\n",
    "        '''\n",
    "        Write out the video with filter to mp4.\n",
    "        '''\n",
    "        array_to_mp4(output_filename, self.des_arr, self.fps)\n",
    "\n",
    "class video_transformer_parallel(video_transformer_base):\n",
    "    '''\n",
    "    This version views the video as an array for easier parallelization.\n",
    "    '''\n",
    "    def __init__(self, path, save_path, file_name, device='cpu',display=False):\n",
    "        video_transformer_base.__init__(self, path, save_path, file_name, \n",
    "                                        device, display)\n",
    "        \n",
    "        self.locations = None\n",
    "        self.des_arr = None\n",
    "\n",
    "        torch.from_numpy(self.video_array).to(self.device)    \n",
    "   \n",
    "    def filter_on_video(self, filter_func, face_detection_model = 'mtcnn', radius = 10):\n",
    "        '''\n",
    "        Apply filter on the video.\n",
    "        '''\n",
    "        self.des_arr = self.video_array.copy()\n",
    "        frame_size = self.video_array.shape[0]\n",
    "        self.locations = self.get_face_locations(face_detection_model)\n",
    "        radius_list = [radius] * frame_size\n",
    "\n",
    "        list(map(filter_func, self.video_array, self.des_arr, self.locations, radius_list))\n",
    "        \n",
    "    def filter_on_video_cuda(self, img_blur_cuda, radius=10, face_detection_model = 'mtcnn'):\n",
    "        '''\n",
    "        Filter on entire video using CUDA for blurring. It specifies block size and controls bounds.\n",
    "        '''\n",
    "        blocksize = (32,32)\n",
    "        k = 1 / (2*radius+1)**2\n",
    "        self.des_arr = self.video_array.copy()\n",
    "        frame_size = self.video_array.shape[0]\n",
    "        self.locations = self.get_face_locations(face_detection_model)\n",
    "        np.save(\"frame_test.out\", self.des_arr[20])\n",
    "        np.save(\"locations_tests.out\", self.locations[20])\n",
    "        for i in range(frame_size):\n",
    "            for location in self.locations[i]:\n",
    "                top, right, bottom, left = location\n",
    "                face = np.ascontiguousarray(self.des_arr[i, top:bottom+1, left:right+1, :])\n",
    "                gridsize = (face.shape[0]//blocksize[0]+1, face.shape[1]//blocksize[1]+1)\n",
    "                blur_face = np.empty_like(face)\n",
    "                img_blur_cuda[gridsize, blocksize](face, blur_face, k, radius)\n",
    "                self.des_arr[i, top:bottom+1, left:right+1, :] = blur_face\n",
    "    \n",
    "    def filter_on_video_cv2(self, cv2_blur, radius=10, face_detection_model = 'mtcnn'):\n",
    "        '''\n",
    "        Filter on entire video using CV2.blur.\n",
    "        '''\n",
    "        k = 1 / (2*radius+1)**2\n",
    "        self.des_arr = self.video_array.copy()\n",
    "        frame_size = self.video_array.shape[0]\n",
    "        self.locations = self.get_face_locations(face_detection_model)\n",
    "        \n",
    "        for i in range(frame_size):\n",
    "            for location in self.locations[i]:\n",
    "                top, right, bottom, left = location\n",
    "                face = self.des_arr[i, top:bottom+1, left:right+1, :]\n",
    "                blur_face = np.empty_like(face)\n",
    "                cv2_blur(src=face, dst=blur_face, ksize=(radius,radius))\n",
    "                self.des_arr[i, top:bottom+1, left:right+1, :] = blur_face   \n",
    "\n",
    "    def filter_on_video_face_only(self, filter_func, radius=10, face_detection_model = 'mtcnn'):\n",
    "        '''\n",
    "        Filter on the entire video, but the filter function only take in the face regions \n",
    "        instead of the entire frame to reduce data size.\n",
    "        '''\n",
    "        k = 1 / (2*radius+1)**2\n",
    "        self.des_arr = self.video_array.copy()\n",
    "        frame_size = self.video_array.shape[0]\n",
    "        self.locations = self.get_face_locations(face_detection_model)\n",
    "\n",
    "        for i in range(frame_size):\n",
    "            for location in self.locations[i]:\n",
    "                top, right, bottom, left = location\n",
    "                face = self.des_arr[i, top:bottom+1, left:right+1, :]\n",
    "                blur_face = np.empty_like(face)\n",
    "                blur_face = filter_func(face=face, blur_face=blur_face, radius=radius)\n",
    "                self.des_arr[i, top:bottom+1, left:right+1, :] = blur_face   \n",
    "    \n",
    "    def mean_blur_convolution(self, image, des_img, locations, radius):\n",
    "        '''\n",
    "        Utilized Scipy convolution to perform blurring effect on faces in one frame.\n",
    "        '''\n",
    "        if len(locations) == 0:\n",
    "              print(\"No faces\")\n",
    "              return\n",
    "          # print(len(locations))\n",
    "        n_neighbor = radius*2+1\n",
    "        height, width, _ = des_img.shape\n",
    "        kernel = np.ones((n_neighbor, n_neighbor))\n",
    "        for (top, right, bottom, left) in locations:\n",
    "            t_ = max(top+radius,0)\n",
    "            b_ = min(bottom-radius, height)\n",
    "            l_ = max(left+radius,0)\n",
    "            r_ = min(right-radius, width)\n",
    "\n",
    "            if t_ >= b_ or l_ >= r_:\n",
    "                continue\n",
    "            sample_area = image[t_-radius:b_+radius+1, l_-radius:r_+radius+1,:].astype(np.uint8)\n",
    "            \n",
    "            # np.save(\"face.out\", sample_area)\n",
    "            red = convolve2d(sample_area[:,:,0], kernel, 'same')\n",
    "            green = convolve2d(sample_area[:,:,1], kernel, 'same')\n",
    "            blue = convolve2d(sample_area[:,:,2], kernel, 'same')\n",
    "\n",
    "            convol_area = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "            des_img[t_-radius:b_+radius+1, l_-radius:r_+radius+1,:] = (convol_area / (n_neighbor**2)).astype(np.uint8)\n",
    "            # cv2.rectangle(des_img, (left, top), (right, bottom), (0, 0, 255), 2)            \n",
    "                    \n",
    "    def mean_blur_convolution_face_only(self, face, blur_face, radius):\n",
    "        '''\n",
    "        Apply blurring with scipy.convolve2d() to faces only.\n",
    "        '''\n",
    "        n_neighbor = radius*2+1\n",
    "        kernel = np.ones((n_neighbor, n_neighbor))\n",
    "        red = convolve2d(face[:,:,0], kernel, 'same')\n",
    "        green = convolve2d(face[:,:,1], kernel, 'same')\n",
    "        blue = convolve2d(face[:,:,2], kernel, 'same')\n",
    "        convol_area = (np.stack([red, green, blue], axis=2)  / (n_neighbor**2)).astype(np.uint8)\n",
    "\n",
    "        return convol_area\n",
    "                    \n",
    "    def get_face_locations(self, face_detection_model):\n",
    "        '''\n",
    "        Get a list of face_locations on entire video.\n",
    "        '''\n",
    "        des_arr = torch.from_numpy(self.video_array.copy()).to(self.device)\n",
    "        \n",
    "        if face_detection_model != 'mtcnn':\n",
    "            locations = list(map(self.face_detection, des_arr))\n",
    "        else:\n",
    "            locations = list(map(self.face_detection_mtcnn, des_arr))    \n",
    "\n",
    "        return locations\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-11T16:44:10.164885Z",
     "start_time": "2021-05-11T16:44:09.999768Z"
    },
    "id": "XlXt1jYUbK8_"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue, Lock\n",
    "class video_transformer_multiprocessing(video_transformer_parallel):\n",
    "    '''\n",
    "    This module aims to use multiprocessing to speed up the blurring.\n",
    "    '''\n",
    "    def __init__(self, path, save_path, file_name,N, device='cpu',display=False):\n",
    "        video_transformer_parallel.__init__(self, path, save_path, file_name, \n",
    "                                        device, display)\n",
    "        self.N = N\n",
    "    def mean_blur_one_location(self, locations, i):\n",
    "        '''\n",
    "        Apply the blurring function to only one face.\n",
    "        '''\n",
    "        for location in locations:\n",
    "            top, right, bottom, left = location\n",
    "            face = self.des_arr[i,top:bottom+1, left:right+1, :]\n",
    "            blur_face = np.empty_like(face)\n",
    "            blur_face = self.mean_blur_convolution_face_only(face=face, blur_face=blur_face, radius=self.radius)\n",
    "            self.des_arr[i, top:bottom+1, left:right+1, :] = blur_face\n",
    "        return self.des_arr\n",
    "    \n",
    "    def mean_blur_one_frame(self, frame, locations):\n",
    "        '''\n",
    "        Apply the blurring function to all faces within one frame.\n",
    "        '''\n",
    "        for location in locations:\n",
    "            # print(location)\n",
    "            top, right, bottom, left = location\n",
    "            face = frame[top:bottom+1, left:right+1, :]\n",
    "            blur_face = np.empty_like(face)\n",
    "            blur_face = self.mean_blur_convolution_face_only(face=face, blur_face=blur_face, radius=self.radius)\n",
    "            frame[top:bottom+1, left:right+1, :] = blur_face\n",
    "        return frame\n",
    "    def mean_blur_some_frame(self, frames, list_locations, queue, idx):\n",
    "        '''\n",
    "        Apply the blurring effect to all faces within multiple frames.\n",
    "        '''\n",
    "        frames_update = []\n",
    "        for i in range(len(frames)):\n",
    "          frame = frames[i]\n",
    "          locations_=list_locations[i]\n",
    "          frame_update = self.mean_blur_one_frame(frame, locations_)\n",
    "          frames_update.append(frame_update)\n",
    "        queue.put((idx,frames_update))\n",
    "        # return frames_update\n",
    "\n",
    "    def filter_on_video_mult(self, filter_func, radius=10, face_detection_model = 'mtcnn'):\n",
    "        '''\n",
    "        Divide the entire video into several parts, and apply blurring in parallel.\n",
    "        '''\n",
    "        self.des_arr = self.video_array.copy()\n",
    "        frame_size = self.video_array.shape[0]\n",
    "        self.locations = self.get_face_locations(face_detection_model)\n",
    "        self.radius = radius\n",
    "\n",
    "        frames_portions = list(np.array_split(self.des_arr, self.N))\n",
    "        locations_portions  = list(np.array_split(self.locations, self.N))\n",
    "        q = Queue()\n",
    "        jobs = []\n",
    "        rets = []\n",
    "\n",
    "        lock = Lock()\n",
    "\n",
    "\n",
    "        for i in range(self.N):\n",
    "            p = Process(target=self.mean_blur_some_frame, args=(frames_portions[i],locations_portions[i], q, i))\n",
    "            p.Daemon = True\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "            \n",
    "        for p in jobs:\n",
    "            ret = q.get() # will block\n",
    "            rets.append(ret)\n",
    "        for p in jobs:\n",
    "            p.join()\n",
    "            \n",
    "        # sort them by the index to restore order\n",
    "        rets.sort(key=lambda x:x[0])\n",
    "\n",
    "        rets = np.array([ret[1] for ret in rets])\n",
    "        \n",
    "        self.des_arr = np.concatenate(rets).astype(np.uint8)\n",
    "        print(self.des_arr.shape)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrEuxHyeFFoh"
   },
   "source": [
    "# We listed some test cases that are described in the report below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T01:30:33.736139Z",
     "start_time": "2021-05-04T01:30:26.258158Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emd2RSWaA_Id",
    "outputId": "51dacc5e-338c-465d-a48b-95ba36877e17"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 56.00291705131531s.\n"
     ]
    }
   ],
   "source": [
    "# CPU Base: use CPU for both face detection (Pytorch) and naive blurring\n",
    "ts = time()\n",
    "case_1 = video_transformer_base(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds_short.mp4',\n",
    "                           display=False)\n",
    "case_1.main_transformation(\"mtcnn\", case_1.mean_blur)\n",
    "case_1.write_to_video(\"cpu_base_peds_short.mp4\")\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xoIgGeXhFefd",
    "outputId": "82306b86-c9b4-438e-8109-d2fb02a30991"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 24.69857692718506s.\n"
     ]
    }
   ],
   "source": [
    "# GPU Base: use GPU for face detection (Pytorch), but CPU for naive blurring\n",
    "ts = time()\n",
    "case_2 = video_transformer_base(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_2.main_transformation(\"mtcnn\", case_2.mean_blur)\n",
    "case_2.write_to_video(\"gpu_base_peds_short.mp4\")\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rRpM8oCsun9H",
    "outputId": "b095b543-31f2-4b17-81bf-8085f907d89e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 16.560893297195435s.\n"
     ]
    }
   ],
   "source": [
    "# GPU + Convolution: use GPU for face detection (Pytorch), and CPU for blurring effects using Scipy convolution\n",
    "ts = time()\n",
    "case_3 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_3.filter_on_video_face_only(case_3.mean_blur_convolution_face_only)\n",
    "case_3.write_to_video(\"peds_short_gpu_parallel_conv.mp4\")\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GCtmDUdU7IaZ"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def img_blur_cuda(img, des_img, k, radius):\n",
    "    '''\n",
    "    numba cuda version of blurring algorithm\n",
    "    '''\n",
    "    i, j = cuda.grid(2)\n",
    "\n",
    "    rows, columns, channel = img.shape\n",
    "    if i >= rows or j >= columns:\n",
    "        return\n",
    "\n",
    "    ra = rows - radius\n",
    "    ca = columns - radius\n",
    "    if i < radius or j < radius or i >= ra or j >= ca:\n",
    "        des_img[i, j, 0] = img[i, j, 0]\n",
    "        des_img[i, j, 1] = img[i, j, 1]\n",
    "        des_img[i, j, 2] = img[i, j, 2]\n",
    "        return\n",
    "\n",
    "    r = 0\n",
    "    g = 0\n",
    "    b = 0\n",
    "    for x in range(-radius, radius + 1):\n",
    "        for y in range(-radius, radius + 1):\n",
    "            i_x = i + x\n",
    "            j_y = j + y\n",
    "            r += img[i_x, j_y, 0] * k\n",
    "            g += img[i_x, j_y, 1] * k\n",
    "            b += img[i_x, j_y, 2] * k\n",
    "    des_img[i, j, 0] = r\n",
    "    des_img[i, j, 1] = g\n",
    "    des_img[i, j, 2] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "m4c2DqtqEi-A",
    "outputId": "d98055c7-ac16-4059-c091-645a1993cd8f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 11.36157751083374s\n"
     ]
    }
   ],
   "source": [
    "# GPU + CUDA: use GPU for face detection (Pytorch), and CUDA for blurring effects\n",
    "start = time()\n",
    "case_4 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_4.filter_on_video_cuda(img_blur_cuda)\n",
    "case_4.write_to_video(\"peds_short_cuda_gpu.mp4\")\n",
    "print(f\"Time used {time() - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "dUQACteGVG8C"
   },
   "outputs": [],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "lXJMC-TzV6WQ"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import cv2\n",
    "from scipy.signal import convolve2d\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "ctypedef np.npy_intp SIZE_t\n",
    "\n",
    "def mean_blur_convolution_cython(np.ndarray[np.uint8_t, ndim=3] image, \n",
    "                                 np.ndarray[np.uint8_t, ndim=3] des_img, \n",
    "                                 np.ndarray locations, \n",
    "                                 double radius):\n",
    "    '''\n",
    "    Utilized Scipy convolution to perform blurring effect.\n",
    "    '''\n",
    "    if len(locations) == 0:\n",
    "          # print(\"No faces\")\n",
    "          return\n",
    "    cdef int n_neighbor = np.int32((radius)*2+1)\n",
    "    cdef int height = des_img.shape[0]\n",
    "    cdef int width = des_img.shape[1]\n",
    "    cdef np.ndarray[int, ndim = 2]kernel = np.ones((n_neighbor, n_neighbor),dtype=np.int32)\n",
    "    cdef int t_, b_, l_, r_, bound_top, bound_bottom, bound_left, bound_right\n",
    "\n",
    "    for (top, right, bottom, left) in locations:\n",
    "        t_ = np.int32(max(top+radius,0))\n",
    "        b_ = np.int32(min(bottom-radius, height))\n",
    "        l_ = np.int32(max(left+radius,0))\n",
    "        r_ = np.int32(min(right-radius, width))\n",
    "\n",
    "        bound_top = np.int32(t_-radius)\n",
    "        bound_bottom = np.int32(b_+radius+1)\n",
    "        bound_left = np.int32(l_-radius)\n",
    "        bound_right = np.int32(r_+radius+1)\n",
    "\n",
    "        if t_ >= b_ or l_ >= r_:\n",
    "            continue\n",
    "        \n",
    "        sample_area = image[bound_top:bound_bottom, \n",
    "                            bound_left:bound_right,:]\n",
    "        \n",
    "        # np.save(\"face.out\", sample_area)\n",
    "        red = convolve2d(sample_area[:,:,np.int32(0)], kernel, 'same')\n",
    "        green = convolve2d(sample_area[:,:,np.int32(1)], kernel, 'same')\n",
    "        blue = convolve2d(sample_area[:,:,np.int32(2)], kernel, 'same')\n",
    "\n",
    "        convol_area = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "        des_img[bound_top:bound_bottom, \n",
    "               bound_left:bound_right,:] = (convol_area / (n_neighbor**2)).astype(np.uint8)\n",
    "        cv2.rectangle(des_img, (left, top), (right, bottom), (0, 0, 255), 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QyOGd1Y_WmgD",
    "outputId": "f416cb21-9081-43aa-cb8b-cab2978da1cb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 16.544543504714966s.\n"
     ]
    }
   ],
   "source": [
    "# GPU + Cythonized Convolution: use GPU for face detection (Pytorch), and use Cython to compile the Scipy Convolution solution\n",
    "ts = time()\n",
    "case_5 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_5.filter_on_video(mean_blur_convolution_cython)\n",
    "case_5.write_to_video(\"peds_short_gpu_parallel_cython.mp4\")\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IA9z64yR6y4I",
    "outputId": "18105c61-6579-453d-ccb8-1f9e275830b3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(52, 1080, 1920, 3)\n",
      "Time used 14.603191375732422s\n"
     ]
    }
   ],
   "source": [
    "# GPU + Multiprocessing Convolution: use GPU for face detection (Pytorch), and use multiprocessing to blur the images\n",
    "start = time()\n",
    "case_6 = video_transformer_multiprocessing(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds_short.mp4',\n",
    "                           N=4,\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_6.filter_on_video_mult(case_6.mean_blur_convolution_face_only)\n",
    "case_6.write_to_video(\"peds_short_gpu_conv_face_only_mult.mp4\")\n",
    "print(f\"Time used {time() - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WBYVytQ47E2q",
    "outputId": "6b6226af-0592-4ff7-b952-9f7c63857f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 11.090066194534302s\n"
     ]
    }
   ],
   "source": [
    "# GPU + CV2 (Benchmark): use GPU for face detection (Pytorch), and use cv2.blur() for blurring \n",
    "start = time()\n",
    "case_cv2 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_cv2.filter_on_video_cv2(cv2.blur)\n",
    "case_cv2.write_to_video(\"peds_short_gpu_cv2.mp4\")\n",
    "print(f\"Time used {time() - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VMcCxdBaSmKv"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "finalized_submission.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
