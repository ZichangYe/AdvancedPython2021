{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpdG99uzA_IN"
   },
   "source": [
    "**Purpose:**\n",
    "This notebook should consist of our methods of detecting faces and adding effects on top on them in a video.\n",
    "\n",
    "**Logs:**\n",
    "- May 3, 2021: Write a class to wrap the face detection and mean blur effect.\n",
    "- May 3, 2021: Include a new face detection model MTCNN that can leverages GPU through PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T02:09:08.855105Z",
     "start_time": "2021-05-04T02:09:02.561765Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FYuvpb2BCLzr",
    "outputId": "c8afb1a3-5a39-4f6c-e468-fbfff2816b46"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting face_recognition\n",
      "  Downloading https://files.pythonhosted.org/packages/1e/95/f6c9330f54ab07bfa032bf3715c12455a381083125d8880c43cbe76bb3d0/face_recognition-1.3.0-py2.py3-none-any.whl\n",
      "Requirement already satisfied: dlib>=19.7 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (19.18.0)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from face_recognition) (1.19.5)\n",
      "Requirement already satisfied: Click>=6.0 in /usr/local/lib/python3.7/dist-packages (from face_recognition) (7.1.2)\n",
      "Collecting face-recognition-models>=0.3.0\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/cf/3b/4fd8c534f6c0d1b80ce0973d01331525538045084c73c153ee6df20224cf/face_recognition_models-0.3.0.tar.gz (100.1MB)\n",
      "\u001b[K     |████████████████████████████████| 100.2MB 55kB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: face-recognition-models\n",
      "  Building wheel for face-recognition-models (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for face-recognition-models: filename=face_recognition_models-0.3.0-py2.py3-none-any.whl size=100566173 sha256=69cf24085a06ab739898d9f3d20445aabd0d70ed13f8a8b1e921961029ba05f6\n",
      "  Stored in directory: /root/.cache/pip/wheels/d2/99/18/59c6c8f01e39810415c0e63f5bede7d83dfb0ffc039865465f\n",
      "Successfully built face-recognition-models\n",
      "Installing collected packages: face-recognition-models, face-recognition\n",
      "Successfully installed face-recognition-1.3.0 face-recognition-models-0.3.0\n",
      "Collecting facenet_pytorch\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/e8/5ea742737665ba9396a8a2be3d2e2b49a13804b56a7e7bb101e8731ade8f/facenet_pytorch-2.5.2-py3-none-any.whl (1.9MB)\n",
      "\u001b[K     |████████████████████████████████| 1.9MB 13.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (0.9.1+cu101)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (1.19.5)\n",
      "Requirement already satisfied: pillow in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (7.1.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from facenet_pytorch) (2.23.0)\n",
      "Requirement already satisfied: torch==1.8.1 in /usr/local/lib/python3.7/dist-packages (from torchvision->facenet_pytorch) (1.8.1+cu101)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (1.24.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2020.12.5)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (3.0.4)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->facenet_pytorch) (2.10)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.8.1->torchvision->facenet_pytorch) (3.7.4.3)\n",
      "Installing collected packages: facenet-pytorch\n",
      "Successfully installed facenet-pytorch-2.5.2\n",
      "Collecting mmcv\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9d/94/b7c17ba89689684ba2029e6d20ddf82bd4b69dfee9b4007bb3d5c0f06ae6/mmcv-1.3.3.tar.gz (282kB)\n",
      "\u001b[K     |████████████████████████████████| 286kB 12.7MB/s \n",
      "\u001b[?25hCollecting addict\n",
      "  Downloading https://files.pythonhosted.org/packages/6a/00/b08f23b7d7e1e14ce01419a467b583edbb93c6cdb8654e54a9cc579cd61f/addict-2.4.0-py3-none-any.whl\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from mmcv) (1.19.5)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.7/dist-packages (from mmcv) (7.1.2)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from mmcv) (3.13)\n",
      "Collecting yapf\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5f/0d/8814e79eb865eab42d95023b58b650d01dec6f8ea87fc9260978b1bf2167/yapf-0.31.0-py2.py3-none-any.whl (185kB)\n",
      "\u001b[K     |████████████████████████████████| 194kB 19.4MB/s \n",
      "\u001b[?25hBuilding wheels for collected packages: mmcv\n",
      "  Building wheel for mmcv (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for mmcv: filename=mmcv-1.3.3-py2.py3-none-any.whl size=412342 sha256=6c9be07cc66b1d375b8adc6658b28dfa604f46f1ec3214b7147a0d6c635684c8\n",
      "  Stored in directory: /root/.cache/pip/wheels/e0/71/ce/a3a6154062e788038be2f044160e5dc70bddcb9076202b5d69\n",
      "Successfully built mmcv\n",
      "Installing collected packages: addict, yapf, mmcv\n",
      "Successfully installed addict-2.4.0 mmcv-1.3.3 yapf-0.31.0\n"
     ]
    }
   ],
   "source": [
    "!pip install face_recognition\n",
    "!pip install facenet_pytorch\n",
    "!pip install mmcv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T17:45:34.219545Z",
     "start_time": "2021-05-04T17:45:34.200672Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OuKTGOu5A_IY",
    "outputId": "0c466147-311f-4aac-a0a7-1f5f7c27ff6f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline \n",
    "import face_recognition\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import imageio\n",
    "from itertools import product\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "# import cupy as cp\n",
    "from time import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T01:30:25.028342Z",
     "start_time": "2021-05-04T01:30:25.023451Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "IWY0mHJSA_Ib",
    "outputId": "cbaf0763-1e05-48d4-8d9c-5591a97d4109"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/drive\n"
     ]
    }
   ],
   "source": [
    "run_on_colab = True\n",
    "if run_on_colab:\n",
    "    from google.colab import drive\n",
    "  \n",
    "    drive.mount('/content/drive', force_remount=True)\n",
    "    path = \"/content/drive/My Drive/AdvancedPython2021\"\n",
    "else:\n",
    "    path = os.getcwd()\n",
    "os.chdir(path)\n",
    "from array_mp4_conversion import array_to_mp4, mp4_to_array\n",
    "file = \"girl.gif\"\n",
    "data_path = os.path.join(path, \"data\", file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "4DKZhQGm61mW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T01:30:25.574502Z",
     "start_time": "2021-05-04T01:30:25.555058Z"
    },
    "code_folding": [],
    "id": "QAfO0x-2A_Ib"
   },
   "outputs": [],
   "source": [
    "'''\n",
    "This script contains the codes that we recently developed, and is used to compile a Cython code.\n",
    "\n",
    "Steps:\n",
    "1. Copy this file to a new .pyx file.\n",
    "2. Compile the code by running python3 setup.py build_ext --inplace.\n",
    "3. Then run test_cython.py.\n",
    "\n",
    "'''\n",
    "import torch\n",
    "import os\n",
    "from itertools import product\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import cv2\n",
    "import matplotlib.patches as patches\n",
    "from IPython.display import clear_output\n",
    "from matplotlib.pyplot import imshow\n",
    "import matplotlib.pylab as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import imageio\n",
    "from itertools import product\n",
    "from facenet_pytorch import MTCNN, InceptionResnetV1\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "import mmcv\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from time import time\n",
    "from array_mp4_conversion import array_to_mp4, mp4_to_array\n",
    "from numba import jit, prange, cuda\n",
    "from collections import defaultdict\n",
    "from scipy.signal import convolve2d\n",
    "# cimport numpy as np\n",
    "# ctypedef np.uint8_t D_TYPE\n",
    "\n",
    "path = os.getcwd()\n",
    "\n",
    "class video_transformer_base:\n",
    "    '''\n",
    "    This is the base of video_transformer, containing basic information about the video.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "                path, \n",
    "                save_path, \n",
    "                file_name, \n",
    "                device='cpu',\n",
    "                display=False):\n",
    "        \n",
    "        self.video_path = os.path.join(path, \"data\", file_name)\n",
    "        self.video_array, self.fps = mp4_to_array(self.video_path)\n",
    "        self.display = display\n",
    "        self.save_path = save_path\n",
    "        self.file_name = file_name\n",
    "        self.num_frames = 0\n",
    "        if device == 'cpu':\n",
    "            self.device = 'cpu'\n",
    "        else:\n",
    "            self.device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    def main_transformation(self, \n",
    "                            face_detection_model, \n",
    "                            filter_effect):\n",
    "        '''\n",
    "        For each frame, do:\n",
    "        1. detect the face;\n",
    "        2. apply the filter;\n",
    "        3. save the processed frame.\n",
    "        '''                    \n",
    "        video_capture = cv2.VideoCapture(self.video_path)\n",
    "        frame_count = 0\n",
    "        output_frames = []\n",
    "        while video_capture.isOpened():    \n",
    "            # Grab a single frame of video\n",
    "            ret, frame = video_capture.read()\n",
    "            # try:\n",
    "            #   frame = torch.from_numpy(frame).to(self.device)\n",
    "            # except:\n",
    "            #   print(ret)\n",
    "\n",
    "            # Bail out when the video file ends\n",
    "            if not ret:\n",
    "                video_capture.release()\n",
    "                break\n",
    "            \n",
    "            frame_count += 1\n",
    "            # print(frame_count)\n",
    "            # print(type(frame))\n",
    "            # detect faces\n",
    "            if face_detection_model != \"mtcnn\":\n",
    "                face_locations = self.face_detection(frame, \n",
    "                                                     model=face_detection_model)\n",
    "            else:\n",
    "                face_locations = self.face_detection_mtcnn(frame)\n",
    "            # print(f\"{len(face_locations)} face(s) detected at frame {frame_count}.\")\n",
    "\n",
    "            # add effect\n",
    "            after_effect_frame = filter_effect(frame, face_locations)\n",
    "\n",
    "            # print(frame_count)\n",
    "            if self.display and frame_count % 2 == 0:\n",
    "               # If faces were found, we will mark it on frame with blue dots\n",
    "                for face_location in face_locations:\n",
    "                    top, right, bottom, left = face_location\n",
    "                    cv2.rectangle(after_effect_frame,(left, top), (right, bottom), (0, 0, 255), 2)\n",
    "                plt.imshow(after_effect_frame)\n",
    "                plt.show()\n",
    "                clear_output(wait=True)\n",
    "            # im = Image.fromarray(after_effect_frame)\n",
    "            # im.save(os.path.join(self.save_path, f\"{self.file_name}_prcs_{frame_count}.png\"))\n",
    "            output_frames.append(after_effect_frame)\n",
    "        self.num_frames = frame_count\n",
    "        self.des_arr = np.array(output_frames)\n",
    "\n",
    "    def face_detection(self, frame):\n",
    "        '''\n",
    "        Face detection with package face_recognition.\n",
    "        Models includes: svm, knn, cnn.\n",
    "        Currently fixed as model='svm' because model ='cnn' is slow.\n",
    "        '''\n",
    "        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        rgb_frame = frame[:, :, ::-1]\n",
    "        face_locations = face_recognition.face_locations(rgb_frame, model='svm')\n",
    "        # print(f\"{len(face_locations)} face(s) detected.\")\n",
    "        \n",
    "        return face_locations\n",
    "    def face_detection_mtcnn(self, frame):\n",
    "        '''\n",
    "        Face detection with package facenet_pytorch.\n",
    "        MTCNN implemented in Pytorch, so also support CUDA.\n",
    "        '''       \n",
    "\n",
    "        mtcnn = MTCNN(keep_all=True, device=self.device)\n",
    "        boxes, _ = mtcnn.detect(frame)\n",
    "        \n",
    "        if boxes is None:\n",
    "            boxes = []\n",
    "            \n",
    "        boxes = np.array([[box[1], box[2], box[3], box[0]] for box in boxes]).astype(np.int)\n",
    "        # print(f\"{len(boxes)} face(s) detected.\")\n",
    "        return boxes\n",
    "    def oil_effect(self, frame):\n",
    "        pass\n",
    "    \n",
    "    def negative_effect(self, frame, locations):\n",
    "        des_img = np.copy(frame)\n",
    "        try:\n",
    "            for location in locations:\n",
    "                t_, r_, b_, l_ = location.astype(int)\n",
    "\n",
    "                des_img[t_:b_,l_:r_] = 255 - frame[t_:b_,l_:r_]\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        return des_img\n",
    "\n",
    "    def mean_blur(self, frame, locations, radius=5):\n",
    "        '''\n",
    "        Apply simple mosaic effect to specified regions. \n",
    "        '''\n",
    "        k = 1 / (radius*2+1)**2\n",
    "        des_img = np.copy(frame)\n",
    "        height, width, _ = des_img.shape\n",
    "        # try:\n",
    "        for location in locations:\n",
    "            top, right, bottom, left = location\n",
    "            t_ = max(top+radius,0)\n",
    "            b_ = min(bottom-radius, height)\n",
    "            l_ = max(left+radius,0)\n",
    "            r_ = min(right-radius, width)\n",
    "            if t_ >= b_ or l_ >= r_:\n",
    "                continue\n",
    "\n",
    "            for i, j in product(range(t_, b_), range(l_, r_)):\n",
    "                kernel = frame[i-radius:i+radius+1, j-radius:j+radius+1, :]\n",
    "                sumed = np.sum(kernel, axis = (0,1)) * k\n",
    "                des_img[i, j] = sumed.astype(np.uint8)\n",
    "        # except:\n",
    "        #     pass\n",
    "        \n",
    "        return des_img    \n",
    "    \n",
    "    # construct transformed gif\n",
    "    def output(self):\n",
    "        images = []\n",
    "        frames_count = list(range(1,self.num_frames))\n",
    "        \n",
    "        for i in frames_count:\n",
    "            try:\n",
    "                images.append(imageio.imread(\n",
    "                    os.path.join(self.save_path, f\"{self.file_name}_prcs_{i}.png\")))\n",
    "            except:\n",
    "                pass\n",
    "        imageio.mimsave(os.path.join(self.save_path, f\"{self.file_name}_prcs.gif\"), images)\n",
    "    \n",
    "    def write_to_video(self, output_filename):\n",
    "        '''\n",
    "        Write out the video with filter to mp4.\n",
    "        '''\n",
    "        array_to_mp4(output_filename, self.des_arr, self.fps)\n",
    "\n",
    "class video_transformer_parallel(video_transformer_base):\n",
    "    '''\n",
    "    This version views the video as an array for easier parallelization.\n",
    "    '''\n",
    "    def __init__(self, path, save_path, file_name, device='cpu',display=False):\n",
    "        video_transformer_base.__init__(self, path, save_path, file_name, \n",
    "                                        device, display)\n",
    "        \n",
    "        self.locations = None\n",
    "        self.des_arr = None\n",
    "\n",
    "        torch.from_numpy(self.video_array).to(self.device)\n",
    "        \n",
    "    def filter_on_video_cuda(self, img_blur_cuda, radius=10, face_detection_model = 'mtcnn'):\n",
    "        blocksize = (32,32)\n",
    "        k = 1 / (2*radius+1)**2\n",
    "        self.des_arr = self.video_array.copy()\n",
    "        frame_size = self.video_array.shape[0]\n",
    "        self.locations = self.get_face_locations(face_detection_model)\n",
    "        np.save(\"frame_test.out\", self.des_arr[20])\n",
    "        np.save(\"locations_tests.out\", self.locations[20])\n",
    "        for i in range(frame_size):\n",
    "            for location in self.locations[i]:\n",
    "                top, right, bottom, left = location\n",
    "                face = np.ascontiguousarray(self.des_arr[i, top:bottom+1, left:right+1, :])\n",
    "                gridsize = (face.shape[0]//blocksize[0]+1, face.shape[1]//blocksize[1]+1)\n",
    "                blur_face = np.empty_like(face)\n",
    "                img_blur_cuda[gridsize, blocksize](face, blur_face, k, radius)\n",
    "                self.des_arr[i, top:bottom+1, left:right+1, :] = blur_face\n",
    "    \n",
    "    def filter_on_video_cv2(self, cv2_blur, radius=10, face_detection_model = 'mtcnn'):\n",
    "        k = 1 / (2*radius+1)**2\n",
    "        self.des_arr = self.video_array.copy()\n",
    "        frame_size = self.video_array.shape[0]\n",
    "        self.locations = self.get_face_locations(face_detection_model)\n",
    "        \n",
    "        for i in range(frame_size):\n",
    "            for location in self.locations[i]:\n",
    "                top, right, bottom, left = location\n",
    "                face = self.des_arr[i, top:bottom+1, left:right+1, :]\n",
    "                blur_face = np.empty_like(face)\n",
    "                cv2_blur(src=face, dst=blur_face, ksize=(radius,radius))\n",
    "                self.des_arr[i, top:bottom+1, left:right+1, :] = blur_face   \n",
    "\n",
    "    def filter_on_video_face_only(self, filter_func, radius=10, face_detection_model = 'mtcnn'):\n",
    "        k = 1 / (2*radius+1)**2\n",
    "        self.des_arr = self.video_array.copy()\n",
    "        frame_size = self.video_array.shape[0]\n",
    "        self.locations = self.get_face_locations(face_detection_model)\n",
    "\n",
    "        for i in range(frame_size):\n",
    "            for location in self.locations[i]:\n",
    "                top, right, bottom, left = location\n",
    "                face = self.des_arr[i, top:bottom+1, left:right+1, :]\n",
    "                blur_face = np.empty_like(face)\n",
    "                blur_face = filter_func(face=face, blur_face=blur_face, radius=radius)\n",
    "                self.des_arr[i, top:bottom+1, left:right+1, :] = blur_face   \n",
    "\n",
    "    # @jit(nopython=False, parallel=True)\n",
    "    def mean_blur_git(self, image, des_img, locations, radius):\n",
    "        '''\n",
    "        mean_blur function with a source and destination image, the logic remains the same.\n",
    "        '''\n",
    "        # radius has to be even\n",
    "        if len(locations) == 0:\n",
    "            print(\"No faces\")\n",
    "            return\n",
    "        # print(len(locations))\n",
    "        k = 1 / (radius*2+1)**2\n",
    "        height, width, _ = des_img.shape\n",
    "        for (top, right, bottom, left) in locations:\n",
    "            t_ = max(top+radius,0)\n",
    "            b_ = min(bottom-radius, height)\n",
    "            l_ = max(left+radius,0)\n",
    "            r_ = min(right-radius, width)\n",
    "\n",
    "            if t_ >= b_ or l_ >= r_:\n",
    "                continue\n",
    "          \n",
    "            for i, j in product(range(t_, b_), range(l_, r_)):\n",
    "                summed = np.sum(image[i-radius:i+radius+1, j-radius:j+radius+1, :], axis = (0,1), dtype=np.uint32)\n",
    "                # print(summed)\n",
    "                # sumed = np.sum(np.sum(kernel, axis= 0, dtype=np.uint32), axis=0)\n",
    "                # sumed = np.sum(kernel, axis=(0,1), dtype=np.uint32)\n",
    "                des_img[i, j, :] = (summed * k).astype(np.uint8)\n",
    "            \n",
    "            cv2.rectangle(des_img, (left, top), (right, bottom), (0, 0, 255), 2)\n",
    "    \n",
    "    def mean_blur_convolution_face_only(self, face, blur_face, radius):\n",
    "        '''\n",
    "        This function only takes in the face portion, intended to decrease the amount of data transfer.\n",
    "        '''\n",
    "        n_neighbor = radius*2+1\n",
    "        kernel = np.ones((n_neighbor, n_neighbor))\n",
    "        red = convolve2d(face[:,:,0], kernel, 'same')\n",
    "        green = convolve2d(face[:,:,1], kernel, 'same')\n",
    "        blue = convolve2d(face[:,:,2], kernel, 'same')\n",
    "        convol_area = (np.stack([red, green, blue], axis=2)  / (n_neighbor**2)).astype(np.uint8)\n",
    "\n",
    "        return convol_area\n",
    "\n",
    "    def mean_blur_convolution(self, image, des_img, locations, radius):\n",
    "        '''\n",
    "        Utilized Scipy convolution to perform blurring effect.\n",
    "        '''\n",
    "        if len(locations) == 0:\n",
    "              print(\"No faces\")\n",
    "              return\n",
    "          # print(len(locations))\n",
    "        n_neighbor = radius*2+1\n",
    "        height, width, _ = des_img.shape\n",
    "        kernel = np.ones((n_neighbor, n_neighbor))\n",
    "        for (top, right, bottom, left) in locations:\n",
    "            t_ = max(top+radius,0)\n",
    "            b_ = min(bottom-radius, height)\n",
    "            l_ = max(left+radius,0)\n",
    "            r_ = min(right-radius, width)\n",
    "\n",
    "            if t_ >= b_ or l_ >= r_:\n",
    "                continue\n",
    "            sample_area = image[t_-radius:b_+radius+1, l_-radius:r_+radius+1,:].astype(np.uint8)\n",
    "            \n",
    "            # np.save(\"face.out\", sample_area)\n",
    "            red = convolve2d(sample_area[:,:,0], kernel, 'same')\n",
    "            green = convolve2d(sample_area[:,:,1], kernel, 'same')\n",
    "            blue = convolve2d(sample_area[:,:,2], kernel, 'same')\n",
    "\n",
    "            convol_area = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "            des_img[t_-radius:b_+radius+1, l_-radius:r_+radius+1,:] = (convol_area / (n_neighbor**2)).astype(np.uint8)\n",
    "            # cv2.rectangle(des_img, (left, top), (right, bottom), (0, 0, 255), 2)            \n",
    "                    \n",
    "                    \n",
    "    def get_face_locations(self, face_detection_model):\n",
    "        '''\n",
    "        get face_locations on entire video as an array.\n",
    "        '''\n",
    "        des_arr = torch.from_numpy(self.video_array.copy()).to(self.device)\n",
    "        \n",
    "        if face_detection_model != 'mtcnn':\n",
    "            locations = list(map(self.face_detection, des_arr))\n",
    "        else:\n",
    "            locations = list(map(self.face_detection_mtcnn, des_arr))    \n",
    "\n",
    "        return locations\n",
    "    \n",
    "    \n",
    "    @jit(nopython=False, parallel=True)\n",
    "    def filter_on_video(self, filter_func, face_detection_model = 'mtcnn', radius = 10):\n",
    "        '''\n",
    "        Apply filter on the video.\n",
    "        '''\n",
    "        self.des_arr = self.video_array.copy()\n",
    "        frame_size = self.video_array.shape[0]\n",
    "        self.locations = self.get_face_locations(face_detection_model)\n",
    "        radius_list = [radius] * frame_size\n",
    "\n",
    "        list(map(filter_func, self.video_array, self.des_arr, self.locations, radius_list))\n",
    "#         for i in prange(frame_size):\n",
    "#             filter_func(self.video_array[i], self.des_arr[i], self.locations[i], radius)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "XlXt1jYUbK8_"
   },
   "outputs": [],
   "source": [
    "from multiprocessing import Process, Queue,Lock\n",
    "class video_transformer_multiprocessing(video_transformer_parallel):\n",
    "    '''\n",
    "    This module aims to use multiprocessing to speed up the blurring.\n",
    "    '''\n",
    "    def __init__(self, path, save_path, file_name,N, device='cpu',display=False):\n",
    "        video_transformer_parallel.__init__(self, path, save_path, file_name, \n",
    "                                        device, display)\n",
    "        self.N = N\n",
    "    def mean_blur_one_location(self, locations, i):\n",
    "        # blur_faces = []\n",
    "        for location in locations:\n",
    "            # print(location)\n",
    "            top, right, bottom, left = location\n",
    "            face = self.des_arr[i,top:bottom+1, left:right+1, :]\n",
    "            blur_face = np.empty_like(face)\n",
    "            blur_face = self.mean_blur_convolution_face_only(face=face, blur_face=blur_face, radius=self.radius)\n",
    "            self.des_arr[i, top:bottom+1, left:right+1, :] = blur_face\n",
    "        return self.des_arr\n",
    "    \n",
    "    def mean_blur_one_frame(self, frame, locations):\n",
    "        for location in locations:\n",
    "            # print(location)\n",
    "            top, right, bottom, left = location\n",
    "            face = frame[top:bottom+1, left:right+1, :]\n",
    "            blur_face = np.empty_like(face)\n",
    "            blur_face = self.mean_blur_convolution_face_only(face=face, blur_face=blur_face, radius=self.radius)\n",
    "            frame[top:bottom+1, left:right+1, :] = blur_face\n",
    "        return frame\n",
    "    def mean_blur_some_frame(self, frames, list_locations, queue, idx):\n",
    "        frames_update = []\n",
    "        for i in range(len(frames)):\n",
    "          frame = frames[i]\n",
    "          locations_=list_locations[i]\n",
    "          frame_update = self.mean_blur_one_frame(frame, locations_)\n",
    "          frames_update.append(frame_update)\n",
    "        queue.put((idx,frames_update))\n",
    "        # return frames_update\n",
    "\n",
    "    def filter_on_video_mult(self, filter_func, radius=10, face_detection_model = 'mtcnn'):\n",
    "        self.des_arr = self.video_array.copy()\n",
    "        frame_size = self.video_array.shape[0]\n",
    "        self.locations = self.get_face_locations(face_detection_model)\n",
    "        self.radius = radius\n",
    "\n",
    "        frames_portions = list(np.array_split(self.des_arr, self.N))\n",
    "        locations_portions  = list(np.array_split(self.locations, self.N))\n",
    "        id = list(range(self.N))\n",
    "        q = Queue()\n",
    "        jobs = []\n",
    "        rets = []\n",
    "        # args = [(self.des_arr[i], self.locations[i]) for i in range(frame_size)]\n",
    "        # with Pool(self.N) as p:\n",
    "        #     results = p.starmap(self.mean_blur_one_frame, args)\n",
    "        lock = Lock()\n",
    "\n",
    "        # for i in range(frame_size):\n",
    "        #   frame = self.des_arr[i]\n",
    "        #   locations_portions = list(np.array_split(self.locations[i], self.N))\n",
    "        for i in range(self.N):\n",
    "            p = Process(target=self.mean_blur_some_frame, args=(frames_portions[i],locations_portions[i], q, i))\n",
    "            # p = Process(target=self.mean_blur_one_frame, args=(frame,locations_portions[i],q))\n",
    "            p.Daemon = True\n",
    "            jobs.append(p)\n",
    "            p.start()\n",
    "        for p in jobs:\n",
    "            ret = q.get() # will block\n",
    "            rets.append(ret)\n",
    "        for p in jobs:\n",
    "            p.join()\n",
    "        # print(len(rets))\n",
    "        rets.sort(key=lambda x:x[0])\n",
    "\n",
    "        # print([ret[0] for ret in rets_sorted])\n",
    "        rets = np.array([ret[1] for ret in rets])\n",
    "        \n",
    "        self.des_arr = np.concatenate(rets).astype(np.uint8)\n",
    "        # print(len(rets))\n",
    "        print(self.des_arr.shape)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "WLlI-SRwgYey"
   },
   "outputs": [],
   "source": [
    "# frame_test = np.load(\"frame_test.out.npy\").astype(np.uint8)\n",
    "# locations_test = np.load(\"locations_tests.out.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TW_8md29gdgp",
    "outputId": "e2c9d267-cb17-4da1-d7ef-d06ad50444eb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[None, None]"
      ]
     },
     "execution_count": 84,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# global des_arr \n",
    "# des_arr= frame_test.copy()\n",
    "# N = 2\n",
    "# chunk = list(np.array_split(locations_test, N))\n",
    "# import itertools\n",
    "# def mean_blur_convolution_face_only(face, blur_face, radius):\n",
    "#     '''\n",
    "#     This function only takes in the face portion, intended to decrease the amount of data transfer.\n",
    "#     '''\n",
    "#     n_neighbor = radius*2+1\n",
    "#     kernel = np.ones((n_neighbor, n_neighbor))\n",
    "#     red = convolve2d(face[:,:,0], kernel, 'same')\n",
    "#     green = convolve2d(face[:,:,1], kernel, 'same')\n",
    "#     blue = convolve2d(face[:,:,2], kernel, 'same')\n",
    "#     convol_area = (np.stack([red, green, blue], axis=2)  / (n_neighbor**2)).astype(np.uint8)\n",
    "#     return convol_area\n",
    "# def mean_blur_one_location(locations):\n",
    "#     # blur_faces = []\n",
    "#     for i in range(len(locations)):\n",
    "#         top, right, bottom, left = locations[i]\n",
    "#         face = des_arr[top:bottom+1, left:right+1, :]\n",
    "#         blur_face = np.empty_like(face)\n",
    "#         blur_face = mean_blur_convolution_face_only(face=face, blur_face=blur_face, radius=10)\n",
    "#         des_arr[top:bottom+1, left:right+1, :] = blur_face\n",
    "\n",
    "\n",
    "# list(map(mean_blur_one_location, chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9gUeag3AiC74"
   },
   "outputs": [],
   "source": [
    "# img = Image.fromarray(des_arr)\n",
    "# img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7RnsJSRndcBN",
    "outputId": "9a2d2f1b-3ec2-462e-a99b-6b1b835baf38"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/dist-packages/numpy/core/_asarray.py:83: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  return array(a, dtype, copy=False, order=order)\n",
      "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:75: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(834, 360, 640, 3)\n",
      "Time used 63.41239666938782s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "case_9 = video_transformer_multiprocessing(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'hamilton_short.mp4',\n",
    "                           N=4,\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_9.filter_on_video_mult(case_9.mean_blur_convolution_face_only)\n",
    "case_9.write_to_video(\"hamilton_short_gpu_conv_face_only_mult.mp4\")\n",
    "print(f\"Time used {time() - start}s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JrEuxHyeFFoh"
   },
   "source": [
    "**1. Base**\n",
    "\n",
    "peds_shorts.mp4 w/ Colab CPU: \n",
    "- 1st: 65.86514019966125s.\n",
    "\n",
    "Peds_short.mp4 w/ Colab GPU\n",
    "- 1st: 32.44067120552063s.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T01:30:33.736139Z",
     "start_time": "2021-05-04T01:30:26.258158Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "emd2RSWaA_Id",
    "outputId": "0021f464-d6c4-4f1d-9ea3-c1b0ce5a014d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 162.5575568675995s.\n"
     ]
    }
   ],
   "source": [
    "ts = time()\n",
    "case_1 = video_transformer_base(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds.mp4',\n",
    "                           display=False)\n",
    "case_1.main_transformation(\"mtcnn\", case_1.mean_blur)\n",
    "case_1.write_to_video()\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xoIgGeXhFefd",
    "outputId": "14701310-71e4-49fb-ef8d-6c5933a651dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 55.120919942855835s.\n"
     ]
    }
   ],
   "source": [
    "ts = time()\n",
    "case_2 = video_transformer_base(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_2.main_transformation(\"mtcnn\", case_2.mean_blur)\n",
    "case_2.write_to_video()\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KiW8_XDGDURx"
   },
   "outputs": [],
   "source": [
    "ts = time()\n",
    "case_3 = video_transformer_base(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'hamilton_clip.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_3.main_transformation(\"mtcnn\", case_3.negative_effect)\n",
    "case_3.output()\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c6cLO-1xEi-A"
   },
   "source": [
    "**2. Parallel**\n",
    "\n",
    "Speeds of Peds_short:\n",
    "  - w/ GPU: 33s\n",
    "  - w/ CPU: 155s\n",
    "  - w/ GPU + convolution blurring: 17s\n",
    "  - w/ GPU + CUDA: 11s\n",
    "\n",
    "Issues:\n",
    "  - Less faced detected:\n",
    "    - Solved by activating iterator ```list(map())```.\n",
    "  - ```array_detection()```\n",
    "    - RAM not enough\n",
    "    - Slower\n",
    "  - ```@jit mean_blur_jit()```\n",
    "    - showing only one location at one time.\n",
    "    - Add the if ```len()==0``` return statement, still doesn'work.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T20:53:37.786185Z",
     "start_time": "2021-05-04T20:53:37.683548Z"
    },
    "id": "11zzHv0LEi-A"
   },
   "outputs": [],
   "source": [
    "ts = time()\n",
    "case_4 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_4.filter_on_video(case_4.mean_blur_git)\n",
    "case_4.write_to_video(\"peds_short_gpu_parallel.mp4\")\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rRpM8oCsun9H"
   },
   "outputs": [],
   "source": [
    "ts = time()\n",
    "case_5 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_5.filter_on_video(case_5.mean_blur_convolution)\n",
    "case_5.write_to_video(\"peds_short_gpu_parallel_conv.mp4\")\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uJFI33nFuw_P"
   },
   "outputs": [],
   "source": [
    "ts = time()\n",
    "case_6 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_6.filter_on_video_cuda()\n",
    "case_6.write_to_video(\"peds_short_gpu_parallel_cuda.mp4\")\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T20:56:03.745231Z",
     "start_time": "2021-05-04T20:56:02.915598Z"
    },
    "id": "ek7dfjLAEi-A"
   },
   "outputs": [],
   "source": [
    "ts = time()\n",
    "case_5 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds.mp4',\n",
    "                           display=False,\n",
    "                           device='cpu')\n",
    "case_5.filter_on_video(case_5.mean_blur)\n",
    "case_5.write_to_video(output_filename = case_5.file_name+\"_cpu_parallel.mp4\")\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T22:40:53.009528Z",
     "start_time": "2021-05-05T22:40:51.974855Z"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "wuIWcdaPjqJx",
    "outputId": "9c53e419-e248-4f0c-d2d0-b8007984c28f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No faces\n",
      "Time used 16.166011333465576s.\n"
     ]
    }
   ],
   "source": [
    "ts = time()\n",
    "case_4 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'hamilton_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_4.filter_on_video(case_4.mean_blur_convolution)\n",
    "case_4.write_to_video(\"hamilton_short_gpu_parallel.mp4\")\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "Mbc6n_XUR0zp",
    "outputId": "2ca35fa9-3433-459a-97a2-108ae926ec9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 35.428462266922s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "case_8 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_8.filter_on_video_face_only(case_8.mean_blur_convolution_face_only)\n",
    "case_8.write_to_video(\"peds_gpu_conv_face_only.mp4\")\n",
    "print(f\"Time used {time() - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "id": "GCtmDUdU7IaZ"
   },
   "outputs": [],
   "source": [
    "@cuda.jit\n",
    "def img_blur_cuda(img, des_img, k, radius):\n",
    "    '''\n",
    "    numba cuda version of blurring algorithm\n",
    "    '''\n",
    "    i, j = cuda.grid(2)\n",
    "\n",
    "    rows, columns, channel = img.shape\n",
    "    if i >= rows or j >= columns:\n",
    "        return\n",
    "\n",
    "    ra = rows - radius\n",
    "    ca = columns - radius\n",
    "    if i < radius or j < radius or i >= ra or j >= ca:\n",
    "        des_img[i, j, 0] = img[i, j, 0]\n",
    "        des_img[i, j, 1] = img[i, j, 1]\n",
    "        des_img[i, j, 2] = img[i, j, 2]\n",
    "        return\n",
    "\n",
    "    r = 0\n",
    "    g = 0\n",
    "    b = 0\n",
    "    for x in range(-radius, radius + 1):\n",
    "        for y in range(-radius, radius + 1):\n",
    "            i_x = i + x\n",
    "            j_y = j + y\n",
    "            r += img[i_x, j_y, 0] * k\n",
    "            g += img[i_x, j_y, 1] * k\n",
    "            b += img[i_x, j_y, 2] * k\n",
    "    des_img[i, j, 0] = r\n",
    "    des_img[i, j, 1] = g\n",
    "    des_img[i, j, 2] = b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "m4c2DqtqEi-A",
    "outputId": "73113020-7f07-4912-d2ca-12d2e5e62a44"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 37.73459982872009s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "case_cuda = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'hamilton_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_cuda.filter_on_video_cuda(img_blur_cuda)\n",
    "case_cuda.write_to_video(\"hamilton_short_cuda_gpu.mp4\")\n",
    "print(f\"Time used {time() - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "id": "4IMwsb1iZ7b4"
   },
   "outputs": [],
   "source": [
    "frame_test = np.load(\"frame_test.out.npy\").astype(np.uint8)\n",
    "locations_test = np.load(\"locations_tests.out.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "dUQACteGVG8C",
    "outputId": "1ba6b168-6ab4-4226-8a29-fd96a1d95140"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Cython extension is already loaded. To reload it, use:\n",
      "  %reload_ext Cython\n"
     ]
    }
   ],
   "source": [
    "%load_ext Cython"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "lXJMC-TzV6WQ"
   },
   "outputs": [],
   "source": [
    "%%cython\n",
    "import cv2\n",
    "from scipy.signal import convolve2d\n",
    "import numpy as np\n",
    "cimport numpy as np\n",
    "ctypedef np.npy_intp SIZE_t\n",
    "# cimport cython\n",
    "# @cython.boundscheck(False)\n",
    "def mean_blur_convolution_cython(np.ndarray[np.uint8_t, ndim=3] image, \n",
    "                                 np.ndarray[np.uint8_t, ndim=3] des_img, \n",
    "                                 np.ndarray locations, \n",
    "                                 double radius):\n",
    "    '''\n",
    "    Utilized Scipy convolution to perform blurring effect.\n",
    "    '''\n",
    "    if len(locations) == 0:\n",
    "          # print(\"No faces\")\n",
    "          return\n",
    "    cdef int n_neighbor = np.int32((radius)*2+1)\n",
    "    cdef int height = des_img.shape[0]\n",
    "    cdef int width = des_img.shape[1]\n",
    "    cdef np.ndarray[int, ndim = 2]kernel = np.ones((n_neighbor, n_neighbor),dtype=np.int32)\n",
    "    cdef int t_, b_, l_, r_, bound_top, bound_bottom, bound_left, bound_right\n",
    "\n",
    "    for (top, right, bottom, left) in locations:\n",
    "        t_ = np.int32(max(top+radius,0))\n",
    "        b_ = np.int32(min(bottom-radius, height))\n",
    "        l_ = np.int32(max(left+radius,0))\n",
    "        r_ = np.int32(min(right-radius, width))\n",
    "\n",
    "        bound_top = np.int32(t_-radius)\n",
    "        bound_bottom = np.int32(b_+radius+1)\n",
    "        bound_left = np.int32(l_-radius)\n",
    "        bound_right = np.int32(r_+radius+1)\n",
    "\n",
    "        if t_ >= b_ or l_ >= r_:\n",
    "            continue\n",
    "        \n",
    "        sample_area = image[bound_top:bound_bottom, \n",
    "                            bound_left:bound_right,:]\n",
    "        \n",
    "        # np.save(\"face.out\", sample_area)\n",
    "        red = convolve2d(sample_area[:,:,np.int32(0)], kernel, 'same')\n",
    "        green = convolve2d(sample_area[:,:,np.int32(1)], kernel, 'same')\n",
    "        blue = convolve2d(sample_area[:,:,np.int32(2)], kernel, 'same')\n",
    "\n",
    "        convol_area = np.stack([red, green, blue], axis=2)\n",
    "\n",
    "        des_img[bound_top:bound_bottom, \n",
    "               bound_left:bound_right,:] = (convol_area / (n_neighbor**2)).astype(np.uint8)\n",
    "        cv2.rectangle(des_img, (left, top), (right, bottom), (0, 0, 255), 2)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "QyOGd1Y_WmgD",
    "outputId": "c51aa16b-0a20-44c0-e8c8-980c4e022982"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 69.33301115036011s.\n"
     ]
    }
   ],
   "source": [
    "ts = time()\n",
    "case_5 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'hamilton_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_5.filter_on_video(mean_blur_convolution_cython)\n",
    "case_5.write_to_video(\"hamilton_short_gpu_parallel_cython.mp4\")\n",
    "print(f\"Time used {time() - ts}s.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "WBYVytQ47E2q",
    "outputId": "111d43ce-9c5b-4da8-886e-c367a7df89de"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time used 10.602014064788818s\n"
     ]
    }
   ],
   "source": [
    "start = time()\n",
    "case_cv2 = video_transformer_parallel(path = path,\n",
    "                           save_path = os.path.join(path, \"data\", \"frames\"),\n",
    "                           file_name = 'peds_short.mp4',\n",
    "                           display=False,\n",
    "                           device='gpu')\n",
    "case_cv2.filter_on_video_cv2(cv2.blur)\n",
    "case_cv2.write_to_video(\"peds_short_cuda_gpu_cv2.mp4\")\n",
    "print(f\"Time used {time() - start}s\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IA9z64yR6y4I"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gRsRlDLY6AU_"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yLIruNsMg84Y"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "video_transformer_test_colab-working-mult.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
